---
title: "Practical Machine Learning Project"
author: "Kevin Payet"
date: "Friday, September 12, 2014"
output:
  html_document:
    keep_md: yes
---

# Getting and preparing the data

```{r}
if(!file.exists("pml-training.csv")) {
    url_train <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
    download.file(url_train, destfile = "./pml-training.csv")
    
    url_test <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
    download.file(url_test, destfile = "./pml-testing.csv")
}

train <- read.csv("pml-training.csv", na.string = c("", "NA"))
```

First, we need to explore and assess the cleanliness of the data: are there missing values, Nans, which variable seem to be usable for our process, which can we safely ignore, ... ?

First, a `summary(train)` shows that there are several features for which most values are missing (I won't show it here because it would simply take too much space). In those cases, almost 98% of the rows have no value. In first approximation, I decided to not use those features, because imputing 19000+ missing values from 400 known values is not a good idea.

What are the features that have too many missing values ?

```{r}
# compute a vector with the fraction of missing values for
missValuesFraction <- unname(apply(X = train, MARGIN = 2, FUN = function(x){1 - (table(is.na(x))[1]/sum(table(is.na(x))))[[1]]}))

unique(missValuesFraction)
```

We can see that either the features have all their values, or ~98% are missing. I will get rid of the latter.

```{r}
missIndex <- which(missValuesFraction > 0)
train <- train[, -missIndex]
```

The 60 remaining features (59 + class) are free of missing values, and ready to be used.

# Features

We have a total of 59 variables to use to predict the class variable. However, we don't have to use them as is, or even use all of them. For example, the X variable, which is just the row name, or the name of the user doesn't seem particularly interesting. At a first glance, it seems that the timestamps value are not to be used. They're is clearly a correlation between the class and the cvtd_timestamp variable; however, the features that should be used are those that come from the measure devices. I will use column 8 to 59 as features to predict the class of the exercise.

```{r}
train <- train[, 8:60]
```

# Training a Support Vector Machine

I used the e1071 package for the machine learning part of this project. I used SVMs with a radial basis kernel. Once the kernel has been chosen, the svm function of the package has 2 hyperparameters that need to be tuned, gamma and cost.

e1071 has a tune function that uses cross-validation or bootstrapping to find the best values for the parameters; however, it is a bit slow. So, I wrote a little function to do the tuning, that can be run on multiple processors. It can be found in the project_functions.R.

```{r eval=FALSE}
source("project_functions.R")

tune_SVM(trainData = list(inputs = train[, 1:52], targets = train[,53]), gammas = c(0.01, 0.1, 0.25, 0.5, 1, 5), costs = c(0.01, 0.1, 1, 10, 25, 50, 100)))
```

For each (gamma, cost) pair, the function uses 10-fold cross-validation to get an estimate of the classification error. The dataset is cut into 10 folds, then an svm is trained on 9 out of the 10 folds, and the resulting model is used to predict the classes of the remaining fold. The comparison between the predicted classes and the true labels gives the classification error. This is done 10 times, each time leaving one different fold out. The classification error for a given pair (gamma, cost) is then simply the average of the errors for each folds configuration. The best parameters are those who give rise to the lowest classification error.

The tuning yielded a gamma value of 0.1 and cost = 25. A last SVM can then be trained on the entire training set using these values.

```{r eval=FALSE}
library(e1071)

# when using SVM, it is important to scale the training inputs first
preObj <- preProcess(train[,1:52], method = c("center", "scale"))
train[,1:52] <- predict(preObj, train[,1:52])

model <- svm(x = train[,1:52], y = train[,53], scale = F, type = "C-classification", kernel = "radial", gamma = 0.1, cost = 25)
```

Prediction for a test set can then be done simply with:

```{r eval=FALSE}
test[,1:52] <- predict(preObj, test[,1:52])

predictions <- predict(model, test) # test here is a data.frame with 52 features
```

# Generalization error

We are also interested in the generalization error of the trained model. I have used cross-validation to obtain an estimate of this error. 

The idea is pretty simple. I divide the training set in k folds. Then, I use a fraction $\frac{k-1}{k}$ as training set and the remaining $\frac{1}{k}$ as validation set. I have k training sets and validation sets. For each of these, I apply the exact same procedure that I've used above:

1. Use the tune_SVM function on the training set to get the best parameters
2. Train an SVM using these parameters on the training set
3. Predict the validation set's targets
4. Compare with the true classes and compute the generalization error

I do this k times, for each fold, and take the average of the generalization errors obtained, and this last value is the estimate of the generalization model of my model.

The function genError in project_functions.R implements the process described above.

```{r eval=FALSE}
source("project_functions.R")

genError(data = train, k = 5) # 5-fold cross-validation
```

```{r echo=FALSE}
print("Generalization error:  0.0126386833913436")
```

The generalization error is a little above 1%. More probably, this is an upper bound to the error, but the true error should of the same order of magnitude.


